{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ent = pd.read_csv('input/output.csv')\n",
    "df_hack = pd.read_csv('input/HackerNoon.csv')\n",
    "df_combined = pd.concat([df_ent, df_hack])\n",
    "df_combined.dropna(inplace = True)\n",
    "df_combined.reset_index(inplace =  True)\n",
    "data = ' '.join(str(x) for x in df_combined['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "sentences = []\n",
    "sentences_label = []\n",
    "\n",
    "def create_sentence(doc):\n",
    "    punctuation = [\".\",\"?\",\"!\",\":\",\"…\"]\n",
    "    sentences = []\n",
    "    sent = []\n",
    "\n",
    "    for word in doc:\n",
    "        if word.text not in punctuation:\n",
    "            if word.text not in (\"\\n\",\"\\n\\n\",'\\u2009','\\xa0', \" \"):\n",
    "                sent.append(word.text.lower())\n",
    "        else:\n",
    "            sent.append(word.text.lower())\n",
    "            if len(sent) > 1:\n",
    "                sentences.append(sent)\n",
    "            sent=[]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDoc = nlp(data)\n",
    "sents = create_sentence(myDoc)\n",
    "# print (sents[:10])\n",
    "sentences = sentences + sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(np.array(sentences).shape[0]):\n",
    "    sentences_label.append(\"ID\" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'hear', 'this', 'sentiment', 'so', 'often', 'from', 'creative', 'entrepreneurs', 'and'] ID0\n",
      "['what', 'goes', 'in', 'and', 'what', 'comes', 'out', 'people', ',', 'resources'] ID1\n",
      "['what', 'can', 'you', 'just', 'not', 'understand', '?'] ID2\n",
      "['why', 'is', 'something', 'the', 'way', 'that', 'it', 'is', '?'] ID3\n",
      "['why', 'is', 'n’t', 'it', 'better', '?'] ID4\n",
      "['ask', 'yourself', 'these', 'questions', 'to', 'identify', 'the', 'problem', 'you', 'are'] ID5\n",
      "['my', 'friends', 'and', 'i', 'created', 'a', '5-question', 'mini', '-', 'survey'] ID6\n",
      "['randy', 'showed', 'the', 'idea', 'to', 'his', 'seal', 'team', 'and', 'then'] ID7\n",
      "['those', 'that', 'have', 'to', 'do', 'with', 'the', 'market', ',', 'founders'] ID8\n",
      "['”', 'asked', 'jackson', 'josh', 'nodded', 'enthusiastically', 'unbeknownst', 'to', 'the', 'duo'] ID9\n"
     ]
    }
   ],
   "source": [
    "for sentence, label in zip(sentences[:10], sentences_label[:10]):\n",
    "    print (sentence[:10], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "\n",
    "        self.labels_list = labels_list\n",
    "\n",
    "        self.doc_list = doc_list\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "\n",
    "            yield gensim.models.doc2vec.LabeledSentence(doc,[self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_doc2vec_model(data, docLabels, size=300, sample=0.000001, dm=0, hs=1, window=10, min_count=0, workers=8,alpha=0.024, min_alpha=0.024, epoch=15, save_file='output/doc2vec.w2v') :\n",
    "    \n",
    "    startime = time.time()\n",
    "    print(\"{0} articles loaded for model\".format(len(data)))\n",
    "\n",
    "    it = LabeledLineSentence(data, docLabels)\n",
    "\n",
    "    model = gensim.models.Doc2Vec(size=size, sample=sample, dm=dm, window=window, min_count=min_count, workers=workers,alpha=alpha, min_alpha=min_alpha, hs=hs) # use fixed learning rate\n",
    "\n",
    "    model.build_vocab(it)\n",
    "\n",
    "    for epoch in range(epoch):\n",
    "\n",
    "        print(\"Training epoch {}\".format(epoch + 1))\n",
    "\n",
    "        model.train(it,total_examples=model.corpus_count,epochs=model.iter)\n",
    "\n",
    "        # model.alpha -= 0.002 # decrease the learning rate\n",
    "        # model.min_alpha = model.alpha # fix the learning rate, no decay\n",
    "    \n",
    "\n",
    "    #saving the created model\n",
    "\n",
    "    model.save(os.path.join(save_file))\n",
    "\n",
    "    print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 articles loaded for model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 2\n",
      "Training epoch 3\n",
      "Training epoch 4\n",
      "Training epoch 5\n",
      "Training epoch 6\n",
      "Training epoch 7\n",
      "Training epoch 8\n",
      "Training epoch 9\n",
      "Training epoch 10\n",
      "Training epoch 11\n",
      "Training epoch 12\n",
      "Training epoch 13\n",
      "Training epoch 14\n",
      "Training epoch 15\n",
      "Training epoch 16\n",
      "Training epoch 17\n",
      "Training epoch 18\n",
      "Training epoch 19\n",
      "Training epoch 20\n",
      "Training epoch 21\n",
      "Training epoch 22\n",
      "Training epoch 23\n",
      "Training epoch 24\n",
      "Training epoch 25\n",
      "Training epoch 26\n",
      "Training epoch 27\n",
      "Training epoch 28\n",
      "Training epoch 29\n",
      "Training epoch 30\n",
      "Training epoch 31\n",
      "Training epoch 32\n",
      "Training epoch 33\n",
      "Training epoch 34\n",
      "Training epoch 35\n",
      "Training epoch 36\n",
      "Training epoch 37\n",
      "Training epoch 38\n",
      "Training epoch 39\n",
      "Training epoch 40\n",
      "Training epoch 41\n",
      "Training epoch 42\n",
      "Training epoch 43\n",
      "Training epoch 44\n",
      "Training epoch 45\n",
      "Training epoch 46\n",
      "Training epoch 47\n",
      "Training epoch 48\n",
      "Training epoch 49\n",
      "Training epoch 50\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "train_doc2vec_model(sentences, sentences_label, size=800, sample=0.0, alpha=0.025, min_alpha=0.001, min_count=0, window=10, epoch=50, dm = 0, hs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 0 : ['i', 'hear', 'this', 'sentiment', 'so', 'often', 'from', 'creative', 'entrepreneurs', 'and', 'small', 'business', 'founders', 'entrepreneurs', 'by', 'nature', 'tend', 'to', 'be', 'people', 'who', 'have', 'big', 'dreams', 'and', 'the', 'guts', 'to', 'follow', 'them', 'on', 'the', 'flip', 'side', ',', 'they', 'often', 'shy', 'away', 'from', ',', 'look', 'down', 'upon', ',', 'or', 'downplay', 'the', 'details', 'that', 'go', 'along', 'with', 'execution', 'you', 'can', 'set', 'big', ',', 'hairy', ',', 'audacious', 'goals', 'for', 'your', 'company', 'goals', 'are', 'critical', 'you', 'can', 'also', 'determine', 'your', 'core', 'values', 'but', 'without', 'understanding', 'what', 'goes', 'into', 'the', 'execution', ',', 'you', 'risk', 'tanking', 'your', 'company', 'one', 'concept', 'to', 'use', 'for', 'a', 'framework', 'around', 'execution', 'is', 'the', 'concept', 'of', 'profitability', 'when', 'you', 'think', 'about', 'the', 'profitability', 'of', 'an', 'idea', ',', 'you', 'need', 'to', 'think', 'about', 'two', 'parts', ':']\n",
      "***\n",
      "sentence 10 : ['jackson', 'is', 'more', 'operational', 'and', 'passionate', 'about', 'automation', 'josh', 'is', 'more', 'strategic', 'and', 'adds', 'a', 'long', '-', 'term', 'mindset', 'to', 'their', 'overall', 'marketing', 'plan', 'together', ',', 'they', 'manage', 'a', 'stable', 'of', 'contractors', '(', 'more', 'on', 'how', 'this', 'exactly', 'works', 'soon', ')', '“', 'having', 'two', 'of', 'us', 'helps', 'smooth', 'the', 'inevitable', 'highs', 'and', 'lows', 'in', 'motivation', 'that', 'come', 'from', 'running', 'a', 'business', ',', '”', 'says', 'jackson', '“', 'when', 'i', '’m', 'down', ',', 'he', '’s', 'up', ',', 'and', 'when', 'he', '’s', 'down', ',', 'i', '’m', 'up', '\\u200a', '—', '\\u200a', 'we', 'keep', 'each', 'other', 'balanced', 'and', 'focused', '”', 'by', 'october', 'of', 'the', 'same', 'year', ',', 'the', 'cofounders', 'grew', 'vivo', 'masks', 'to', '$', '30,000', 'a', 'month', 'in', 'sales', 'by', 'october', 'they', 'did', 'n’t', 'pay', 'themselves', 'a', 'dime', ',', 'but', 'rather', 'reinvested', 'the', 'revenue', 'into', 'growing', 'inventory', 'and', 'marketing', 'until', 'they', 'fully', 'stopped', 'the', 'real', 'estate', 'business', 'in', 'the', 'following', 'spring', 'the', 'key', 'to', 'setting', 'up', 'an', 'effective', 'digital', 'marketing', 'funnel', 'has', 'nothing', 'to', 'do', 'with', 'tweaking', 'online', 'advertisement', 'settings', ',', 'copy', ',', 'and', 'the', 'perfect', 'software', 'setup', 'to', 'form', 'a', '“', 'well', '-', 'oiled', 'machine', '”', 'where', 'every', '$', '1', 'spent', 'nets', '$', '1', '50', 'in', 'revenue', ',', 'says', 'jackson', '“', 'it', '’s', 'much', 'more', 'human', 'than', 'that', ',', 'especially', 'at', 'the', 'beginning', '”', 'jackson', 'says', 'spend', 'most', 'of', 'your', 'time', 'finding', 'the', 'absolute', 'best', 'possible', 'product', 'first', '“', 'if', 'you', 'put', 'most', 'of', 'your', 'effort', 'into', 'sourcing', 'the', 'best', 'possible', 'product', 'on', 'the', 'front', 'end', ',', 'it', 'will', 'make', 'your', 'marketing', 'down', 'the', 'road', 'much', 'easier', ',', '”', 'he', 'says', 'he', 'spoke', 'about', 'how', 'his', 'handmade', 'masks', 'have', 'high', '-', 'end', 'artisanal', 'italian', 'origins', '“', 'i', 'can', 'say', 'with', 'full', 'integrity', 'that', 'i', 'sell', 'the', 'highest', '-', 'quality', 'masquerade', 'masks', 'in', 'the', 'world', '”', 'once', 'you', '’ve', 'found', 'a', '“', 'really', 'nice', '”', 'product', 'to', 'sell', 'online', ',', 'shift', 'your', 'thinking', 'from', 'short', '-', 'term', '\\u200a', '—', '\\u200a', '“', 'how', 'do', 'i', 'get', 'this', 'to', 'make', 'money', 'next', 'month', '?']\n",
      "***\n",
      "sentence 20 : ['”', 'jackson', 'wondered', 'he', 'applied', 'the', 'same', 'filter', 'that', 'landed', 'him', 'in', 'masquerade', 'masks', 'and', 'men', '’s', 'suspenders', ':']\n",
      "***\n",
      "sentence 30 : ['to', 'be', 'successful', 'in', 'business', 'does', 'not', 'mean', 'changing', 'the', 'world', 'it', 'means', 'meeting', 'a', 'need', '(', 'regardless', 'of', 'size', ')', 'well', 'and', 'dependably', 'over', 'time', 'sell', 'something', 'if', 'it', '’s', 'a', 'product', ',', 'make', 'sure', 'its', 'quality', ',', 'then', 'sell', 'it', 'at', 'a', 'fair', 'price', ',', 'and', 'make', 'sure', 'the', 'breakeven', 'point', 'is', 'doable', 'for', 'you', 'as', 'the', 'business', 'owner', 'if', 'it', '’s', 'a', 'service', ',', 'make', 'sure', 'there', 'is', 'added', 'value', ',', 'as', 'in', 'you', 'are', 'very', 'good', 'and', 'knowledgeable', 'in', 'your', 'field', ',', 'and', 'the', 'service', 'provided', 'is', 'unique', ',', 'at', 'least', 'in', 'your', 'market', 'multiple', 'revenue', 'streams', 'do', 'not', 'partner', 'study', 'the', 'market', 'be', 'very', ',', 'very', 'careful', 'with', 'who', 'you', 'hire', 'how', 'to', 'do', 'well', 'in', 'business', '“', 'google', 'cloud', 'platform', 'and', 'firebase', 'give', 'hacker', 'noon', 'the', 'flexibility', 'to', 'craft', 'a', 'custom', 'publishing', 'platform', 'optimized', 'for', 'technologists', ',', '”', 'said', 'hacker', 'noon', 'interim', 'cto', 'dane', 'lyons', '“', 'our', 'serverless', 'infrastructure', 'will', 'generate', 'static', 'content', 'and', 'pipe', 'it', 'into', 'the', 'low', 'latency', ',', 'low', '-', 'cost', 'google', 'cloud', 'cdn', 'we', '’re', 'excited', 'to', 'focus', 'on', 'important', 'product', 'details', 'and', 'worry', 'less', 'about', 'devops', 'and', 'cost', 'optimizations', '”', 'as', 'a', 'startup', 'working', 'to', 'free', 'ourselves', 'from', 'platform', 'dependency', ',', 'it', '’s', 'humbling', 'to', 'have', 'google', 'support', 'our', 'own', 'infrastructure', 'these', 'resources', 'make', 'our', 'future', 'more', 'secure', ',', 'our', 'monthly', 'burn', 'rate', 'more', 'manageable', ',', 'and', 'ultimately', 'provide', 'a', 'stronger', 'partner', 'for', 'serving', 'high', 'volumes', 'of', 'traffic', 'in', 'the', 'long', '-', 'term', 'we', '’re', 'very', 'excited', 'about', 'launching', 'the', 'next', 'iteration', 'of', 'hacker', 'noon', 'with', 'google', 'cloud', 'platform', 'and', 'firebase', '!']\n",
      "***\n",
      "sentence 40 : ['google', 'founders', 'collection', 'comes', 'to', 'computer', 'history', 'museum', 'by', 'computer', 'history', 'museum', 'google', '+', 'machine', 'learning', '+', 'getting', 'started', '=', 'awesomeness', ',', 'and', 'good', 'cucumbers', 'by', 'beehyve', 'instance', 'segmentation', 'in', 'google', 'colab', 'with', 'custom', 'dataset', 'by', 'romroc', 'train', 'your', 'machine', 'learning', 'models', 'on', 'google', '’s', 'gpus', 'for', 'free', '\\u200a', '—', '\\u200a', 'forever', 'by', 'nick', 'bourdakos', 'implement', 'google', 'maps', 'in', 'reactjs', 'by', 'mohammed', 'chisti', 'let', '’s', 'make', 'reusable', 'web', 'components', ':']\n",
      "***\n",
      "sentence 50 : ['you', 'can', 'build', 'this', 'model', 'using', 'multiple', 'lstm', 'layers', ',', 'with', 'the', 'basic', 'lstm', 'cells', 'assigning', 'each', 'layer', 'with', 'the', 'specified', 'number', 'of', 'cells', ',', 'as', 'shown', 'in', 'the', 'following', 'diagram', ':']\n",
      "***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 60 : ['here', ',', 'you', 'can', 'see', 'that', 'the', 'model', 'has', 'learned', 'in', 'the', 'way', 'it', 'has', 'generated', 'the', 'paragraphs', 'and', 'sentences', 'with', 'appropriate', 'spacing', 'it', 'still', 'lacks', 'perfection', 'and', 'also', 'does', 'n’t', 'make', 'sense', 'seeing', 'signs', 'of', 'success', '\\u200a', '—', '\\u200a', 'the', 'first', 'task', 'is', 'to', 'create', 'a', 'model', 'that', 'can', 'learn', ',', 'and', 'then', 'the', 'second', 'one', 'is', 'used', 'to', 'improve', 'on', 'that', 'model', 'this', 'can', 'be', 'obtained', 'by', 'training', 'the', 'model', 'with', 'a', 'larger', 'training', 'dataset', 'and', 'longer', 'training', 'durations', 'if', 'you', 'found', 'this', 'article', 'interesting', ',', 'you', 'can', 'explore', 'python', 'deep', 'learning', 'projects', 'to', 'master', 'deep', 'learning', 'and', 'neural', 'network', 'architectures', 'using', 'python', 'and', 'keras', 'python', 'deep', 'learning', 'projects', 'imparts', 'all', 'the', 'knowledge', 'needed', 'to', 'implement', 'complex', 'deep', 'learning', 'projects', 'in', 'the', 'field', 'of', 'computational', 'linguistics', 'and', 'computer', 'vision', 'data', 'pre', '-', 'processing', 'defining', 'the', 'model', 'training', 'the', 'deep', 'tensorflow', '-', 'based', 'lstm', 'model', 'inference', 'output', 'today', ',', 'i', 'was', 'asked', 'by', 'my', 'employer', 'for', 'my', 'github', 'password', 'this', 'is', 'something', 'i', '’m', 'not', 'willing', 'to', 'give', 'out', ',', 'especially', 'since', 'i', 'work', 'on', 'other', 'projects', '(', 'outside', 'of', 'work', ')', 'and', 'am', 'not', 'willing', 'to', 'compromise', 'anyone', '’s', 'data', 'does', 'anyone', 'have', 'any', 'advice', 'on', 'how', 'to', 'respond', 'to', 'this', 'request', '?']\n",
      "***\n",
      "sentence 70 : ['i', '’m', 'the', 'co', '-', 'founder', 'of', 'godo', ',', 'a', 'data', 'design', 'firm', 'working', 'to', 'help', 'businesses', 'make', 'their', 'data', 'useful', 'feel', 'free', 'to', 'reach', 'out', 'here', ',', 'on', 'twitter', ',', 'or', 'linkedin', 'since', 'big', 'data', 'has', 'proven', 'its', 'usability', 'in', 'retail', ',', 'marketing', ',', 'and', 'other', 'areas', ',', 'healthcare', 'managers', 'are', 'now', 'thinking', 'about', 'how', 'to', 'reap', 'the', 'benefits', 'of', 'this', 'technology', 'for', 'their', 'own', 'problems', 'artificial', 'intelligence', 'in', 'the', 'form', 'of', 'natural', 'language', 'processing', '(', 'nlp', ')', 'can', 'improve', 'critical', 'aspects', 'of', 'the', 'patient', '-', 'doctor', 'relationship', 'and', 'can', 'even', 'go', 'beyond', 'this', ',', 'simplifying', 'the', 'process', 'of', 'insurance', 'payment', 'the', 'expected', 'advancement', 'comes', 'from', 'making', 'the', 'clinical', 'documentation', 'more', 'accessible', 'through', 'automatic', 'indexing', ',', 'thus', 'adding', 'search', '-', 'ability', 'another', 'growth', 'direction', 'is', 'the', 'automatic', 'voice', '-', 'to', '-', 'text', 'feature', 'which', 'will', 'enable', 'the', 'creation', 'of', 'automated', 'digital', 'records', 'while', 'allowing', 'medical', 'staff', 'to', 'focus', 'their', 'attention', 'on', 'patients', 'instead', 'of', 'writing', 'this', 'is', 'solving', 'the', 'problem', 'that', 'more', 'than', '83', '%', 'of', 'physicians', 'have', 'reported', ':']\n",
      "***\n",
      "sentence 80 : ['the', 'firebase', 'story', 'by', 'founder', 'collective', 'infinite', 'scrolling', 'in', 'firebase', 'by', 'linas', 'm', 'introduction', 'to', 'firebase', 'by', 'geekyants', 'prototyping', 'with', 'firebase', 'by', 'david', 'kerr', 'flutter', '\\u200a', '—', '\\u200a', '5', 'reasons', 'why', 'you', 'may', 'love', 'it', 'by', 'paulina', 'szklarska', 'i', 'love', 'you', 'flutter', 'by', 'shalom', 'yerushalmy', 'what', 'are', 'the', 'google', 'cloud', 'platform', '(', 'gcp', ')', 'services', '?']\n",
      "***\n",
      "sentence 90 : ['that', 'leads', 'to', 'the', 'second', 'feature', 'of', '5', 'g', '(', 'next', 'item', ')', 'to', 'make', 'sure', 'that', 'signals', 'are', 'able', 'to', 'travel', 'farther', 'without', 'fading', ',', '“', 'small', 'cell', '”', 'towers', 'using', 'a', 'dense', 'deployment', 'will', 'be', 'installed', 'within', 'an', 'area', 'to', 'handle', 'the', 'signaling', 'they', 'will', 'be', 'much', 'smaller', 'than', 'your', 'typical', 'cellular', 'tower', 'these', 'small', 'cell', 'towers', 'will', 'be', 'placed', 'within', 'a', 'distance', 'of', 'no', 'less', 'than', '200', 'feet', 'and', 'no', 'more', 'than', '1,000', 'feet', 'apart', ',', 'so', 'they', 'are', 'pretty', 'close', 'to', 'each', 'other', 'the', 'advantage', 'to', 'being', 'smaller', 'is', 'that', 'there', 'is', 'more', 'versatility', 'to', 'where', 'they', 'can', 'be', 'installed', 'they', 'can', 'be', 'put', 'on', 'the', 'side', 'of', 'buildings', ',', 'utility', 'poles', ',', 'apartment', 'rooftops', 'just', 'to', 'give', 'a', 'few', 'examples', 'the', 'small', 'cells', 'then', 'transmit', 'and', 'receive', 'data', 'on', 'the', '5', 'g', 'network', 'covering', 'a', 'certain', 'area', 'so', 'the', 'idea', 'of', '5', 'g', 'is', 'replacing', 'high', 'power', ',', 'low', 'frequency', 'towers', 'with', 'low', 'power', ',', 'high', 'frequency', 'small', 'cells', 'that', 'communicate', 'with', 'a', 'base', 'station', 'to', 'handle', 'data', 'traffic', 'signaling', ',', '“', 'beamforming', '”', 'will', 'be', 'used', 'it', 'determines', 'the', 'most', 'efficient', 'data', 'delivery', 'route', 'in', 'a', '5', 'g', 'network', 'beamforming', 'actually', 'sends', 'the', 'data', 'from', 'the', 'small', 'cell', 'directly', 'to', 'the', 'user', 'since', 'the', 'signal', 'is', 'more', 'concentrated', ',', 'it', 'reduces', 'interference', 'as', 'well', 'like', 'its', 'predecessor', '4', 'g', ',', 'a', '5', 'g', 'network', 'makes', 'use', 'of', 'packet', 'switching', 'over', 'an', 'ip', 'network', 'for', 'data', 'delivery', 'mimo', 'or', '“', 'multiple', '-', 'input', 'multiple', '-', 'output', '”', 'allows', 'more', 'signals', 'to', 'be', 'sent', 'and', 'received', 'at', 'any', 'given', 'moment', 'this', 'is', 'implemented', 'by', 'installing', 'more', 'antennas', 'as', 'an', 'array', 'in', 'a', 'small', 'cell', 'the', 'problem', 'of', 'having', 'so', 'many', 'antennas', 'installed', 'is', 'addressed', 'by', 'beamforming', 'with', 'mimo', ',', 'a', 'base', 'station', 'can', 'send', 'and', 'receive', 'more', 'signals', 'to', 'boost', 'the', 'capacity', 'of', 'a', '5', 'g', 'network', 'by', 'a', 'factor', 'of', '22', ',', 'first', 'reported', 'by', 'engineers', 'at', 'the', 'university', 'of', 'bristol', 'and', 'sweden', '’s', 'lund', 'university', 'fact', ':']\n",
      "***\n",
      "sentence 100 : ['let', '’s', 'find', 'out', 'some', 'executives', 'still', 'hold', 'to', 'an', 'opinion', 'that', 'outsourcing', 'numerous', 'functions', 'is', 'an', 'ill', '-', 'fated', 'strategy', 'which', 'may', 'lead', 'to', 'complete', 'loss', 'of', 'control', 'and', 'business', 'agility', 'yet', ',', 'the', 'quality', 'of', 'software', 'outsourcing', 'services', 'greatly', 'depends', 'on', 'the', 'specification', 'of', 'your', 'provider', 'in', 'this', 'respect', ',', 'outsourcing', 'vendors', 'generally', 'fall', 'within', 'three', 'categories', ':']\n",
      "***\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Required argument 'file' (pos 2) not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-f629909d18e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0msentences_vector_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'output'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sentences_vector_800_a025_ma001_s10000.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences_vector_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mcPickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: Required argument 'file' (pos 2) not found"
     ]
    }
   ],
   "source": [
    "from six.moves import cPickle\n",
    "d2v_model = gensim.models.doc2vec.Doc2Vec.load('output/doc2vec.w2v')\n",
    "\n",
    "sentence_vector = []\n",
    "t = 10\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    if i%t == 0 : \n",
    "        print(\"sentence\", i, \":\", sentences[i])\n",
    "        print(\"***\")\n",
    "    sent = sentences[i]\n",
    "    sentence_vector.append(d2v_model.infer_vector(sent, alpha = 0.025, min_alpha = 0.001, steps = 10000))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_vector_file = os.path.join('output', 'sentences_vector_800_a025_ma001_s10000.pkl')\n",
    "\n",
    "with open(os.path.join(sentences_vector_file), 'wb') as f:\n",
    "    cPickle.dump(sentence_vector, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new sequence:  0\n",
      "  1 th vector for this sequence. Sentence  ID0 (vector dim =  3 )\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'I'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-e36feba73598>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;34m'th vector for this sequence. Sentence '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"(vector dim = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\")\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0msenty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentences_label\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnb_sequenced_sentences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mvecty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence_vector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnb_sequenced_sentences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'I'"
     ]
    }
   ],
   "source": [
    "nb_sequenced_sentences = 15\n",
    "vector_dim = 500\n",
    "\n",
    "X_train = np.zeros((len(sentences), nb_sequenced_sentences, vector_dim), dtype = np.float)\n",
    "y_train = np.zeros((len(sentences), vector_dim), dtype = np.float)\n",
    "\n",
    "t = 10\n",
    "for i in range(len(sentences) - nb_sequenced_sentences - 1):\n",
    "    if i%t == 0:\n",
    "        print('new sequence: ', i)\n",
    "    \n",
    "    for k in range(nb_sequenced_sentences):\n",
    "        sent = sentences_label[i+k]\n",
    "        vect = sentences_label[i+k]\n",
    "        \n",
    "        if i%t == 0:\n",
    "            print (' ', k+1 ,'th vector for this sequence. Sentence ', sent, \"(vector dim = \", len(vect), \")\")\n",
    "        for j in range(len(vect)):\n",
    "            X_train[i, k, j] = vect[j]\n",
    "    senty = sentences_label[i + nb_sequenced_sentences]\n",
    "    vecty = sentence_vector[i + nb_sequenced_sentences]\n",
    "    if i%t == 0:\n",
    "        print(' y vector for this sequence ', senty, ': (vector dim = ', len(vecty), ')')\n",
    "    for j in range(len(vecty)):\n",
    "        y_train[i, j] = vecty[j]\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
